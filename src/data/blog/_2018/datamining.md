---
title: Data Mining of Deng Era
pubDatetime: 2018-04-15T00:55:43Z
description: "期中作业之一是写一篇邓小平时代的读后感，说实话这种书是实在没空读了，虽然粗略地翻了几章，十分吸引人，但是这两年真的越来越讨厌写文科式的论文，瞎胡诌凑字数曾经也是我作为理科生的优势，但是这一两年对这种风格的文章：东拼西凑，无病呻吟，迷茫又自负的写作非常地厌恶。因为就想玩点花样，做点简单的文本数据挖掘凑凑字数，虽然多花了很多时间，但是毕竟很有意思，有意思的事情就不算浪费时间对吧，没有意思的事情，哪怕一分钟也是对生命的浪费呢。"
author: "James Chen"
tags: ["codes", "data science", "statistics", "data mining", "matplotlib"]
categories: ["techniques"]
draft: false
---

期中作业之一是写一篇邓小平时代的读后感，说实话这种书是实在没空读了，虽然粗略地翻了几章，十分吸引人，但是这两年真的越来越讨厌写文科式的论文，瞎胡诌凑字数曾经也是我作为理科生的优势，但是这一两年对这种风格的文章：东拼西凑，无病呻吟，迷茫又自负的写作非常地厌恶。因为就想玩点花样，做点简单的文本数据挖掘凑凑字数，虽然多花了很多时间，但是毕竟很有意思，有意思的事情就不算浪费时间对吧，没有意思的事情，哪怕一分钟也是对生命的浪费呢。

中文分词是个很好玩的事情，但是jieba和THULAC之类的工具已经把中文分词和词性标注之类的变得很简单，最折腾的，花了我很久时间的是这本中文材料。。。matplotlib本身不支持英文绘图，python的encoding方式也让我折腾了很久，竟然做了很久装卸各种包的工作，，，直到最后奇葩的matplotlib就是找不到字体，不管在本地还是在几个服务器上竟然都不行，于是只好测试好代码让斌斌帮忙在他的账户跑一下。下面简单记一下过程和代码，最后再把自己胡乱拼凑的论文也扔上。

代码也放到[GitHub](https://github.com/james20141606/somethingmore/datamining_dxp)上了,里面附有jupyter版本的代码和可以直接运行产生各种类型图片的代码。


# 对邓小平时代的分词与词频统计
## 数据准备

**convert to UTF-8 format**
```
file ip.txt
#use vim to change encoding format
:set fileencoding=utf-8
```
首先是找到txt版本的邓小平时代资源，用utf-8编码，方便后续处理。

## 分词与词频统计
想做词频统计分析，就得对文本进行分词，中文和英文不同，英文单词是孤立的，而中文单词需要人工分开，这里我找了一个比较经典的分词方法，**Jieba分词**，对整本书进行了分词处理，把每句话都给分开，存储了各个名词，并且顺便统计了一下出现频次前10000的所有词语。因此我通过代码可以获取以下**两个文件**：

被分词分开的全书“词汇”，按顺序一个个存储起来，以及对各个词汇出现频次的统计文件。接下来就可以对数据进行进一步的分析。

**Use Jieba for Chinese words partition**
```
import os, codecs  
import jieba  
import numpy as np
from collections import Counter 
import matplotlib.pylab as plt
plt.style.use('ggplot')
import pandas as pd
import matplotlib
plt.rcParams['font.style'] = u'normal'
plt.rcParams['font.family'] = u'Microsoft YaHei'
with codecs.open('output.txt', 'r', 'utf8') as f:  
    txt = f.read() 
seg_list = jieba.cut(txt) 
c = Counter()  
for x in seg_list:  
    if len(x)>1 and x != '\r\n':  
        c[x] += 1
np.savetxt('count10000.txt',np.array(c.most_common(10000)),fmt='%s')
data = np.loadtxt('count10000.txt',dtype='str')
with codecs.open('output.txt', 'r', 'utf8') as f:  
    txt = f.read() 
wordlist = np.array(txt.split(' '))
#wordlist.shape
countlist = []
for i in range(10000):
    countlist.append(data[i,0]+': '+str(data[i,1]))
pd.DataFrame(np.array(countlist)[:200].reshape(20,10)).head()
```
然后选取最靠前的200个词语制出来一张表格，从这个表格里还是可以看出一些信息量的，还是很有趣的。比如毛泽东作为中国近现代史的第一人物，是本书除了邓小平之外绕不开的第二号人物。干部一词也反复出现，在中国这是个非常重要的词语，很多东西都取决于干部之间的博弈和关系。北京作为政治中心和中国的代名词，自然也反复出现，而国家和地区层面，美国，苏联、日本和中国台湾也榜上有名，广东作为非常重要的试验地点，被提及的频率也相当的高。人物上，胡耀邦、陈云、赵紫阳、周恩来也都出现了多次。军队、学生等关键词也出现次数不少。

除此之外还有年份也引人关注，比如1975、1977、1979、1980、1989等关键节点也都帮上有名。

![Markdown](http://i4.bvimg.com/640680/f405c02d19042f6b.png)

## 结果可视化
**除此之外，我还对一些非常重要的关键词画了一些可视化的图，这里选取一些放上来。**

### bar plot
```
namelist = [u'邓小平',u'中国',u'毛泽东',u'工作',u'干部',u'问题',u'北京',u'美国',u'领导人',u'会议',u'经济',u'关系',u'香港',u'1975',u'领导',u'胡耀邦',u'苏联',u'政治',u'支持',
u'军队',u'陈云',u'政策',u'赵紫阳',u'周恩',u'讲话',u'学生',u'华国锋',u'改革',u'日本']
index_25 = [0,1,3,4,6,7,8,11,12,14,16,20,21,23,26,27,28,31,33,37,39,40,42,46,47,48,49]
count = 27
fig,ax=plt.subplots(1,figsize=(20,10))
ax.bar(range(count),data[index_25,1].astype('int'),color = 'b')
#ax.bar(range(count),data[:count,1].astype('int'))
ax.set_xticks(range(count))
ax.set_xticklabels(namelist)
#plt.savefig('tst.png')
ax.set_title(str(count)+' key words frequency in book')
```
比如这个显示前二十个关键词的bar plot，可以发现相当有趣的现象，在一本书中的关键词分布竟然也挺像幂率分布，某两三个关键词频次非常高，然后是一堆比较重要的关键词，这个也很有趣。
![Markdown](http://i4.bvimg.com/640680/9190c33461d494bd.png)

### fluctuation 
接下来我还画了重要词汇再不同章节的变化图。这个的难点是要先获取每一章的起始和结束的位置（不是书本的页码，而是自己分割出来的“单词表”上的位置）
```
chapterind = np.array([16590,  31267,  54053,69769,  90171, 104745,121010, 138136,  147048,  161724,170963,  193593, 206502, 214129,230193, 
                245828,  260400,285768, 303284, 324922, 337101, 349241, 362426, 377184])-1
def count_frequent(chap):
    freqlist =[]
    if chap <23:
        for i in range(27):
            freqlist.append(np.where(wordlist[chapterind[chap]:chapterind[chap+1]] ==namelist[i])[0].shape[0])
    else:
        for i in range(27):
            freqlist.append(np.where(wordlist[chapterind[chap]:] ==namelist[i])[0].shape[0])
    return np.array(freqlist)

freq_var = np.ndarray([24,27])
for i in range(24):
    freq_var[i] = count_frequent(i)

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
transformed = scaler.fit_transform(freq_var)

fig,ax=plt.subplots(1,figsize =(10,10))
ax.matshow(transformed.T ,cmap ='jet')
ax.set_title('27 key words fluctuation in 24 chapters')
ax.set_xticks(range(24))
ax.set_yticks(range(27))
ax.set_yticklabels(namelist)
```
#### heatmap
经过一番折腾就可以统计出来27个关键词在24章的词频的变化，然后先画了一个**heatmap热力图**，这里为了避免某些关键词，比如邓小平出现频次太多影响到其他关键词的颜色，对每行做了归一化的处理（Minmaxscale）。
![Markdown](http://i4.bvimg.com/640680/231ecc794e04d4d7.png)

这个图每一行是一个关键词，每一列是一章。信息量也是蛮大的，比如毛泽东在前面几章出现频次极其的高，后面由于趋势的原因，提的渐渐少了很多，变化相当明显。再比如支持一词，在后面的章节出现很多，可以推理强调邓小平受到他人支持以及支持他人推进改革的次数不少。学生这个关键词在19-21章出现非常多，闭着眼睛也知道这几张在讲什么（政治的潮起潮落、北京之春和天安门事件）。总之用heatmap图的方法也可以粗略地对关键词，尤其是关键词在每章中的变化做一些分析，更加细致的分析可以通过索引回一开始产生的全书词汇找到前后文再仔细看。

#### 折线图
接下来又绘制了一个更加直观的折线图，展示不同关键词在不同章节的变化情况，但是由于混杂在一起，可能不如热力图易读。
```
fig,ax=plt.subplots(1,figsize =(20,10))
#ax.plot(freq_var[:,:10])
count =10
for x,y in zip(freq_var[:,:count].T,namelist[:count]):
    plt.plot(x,label =y)
plt.title(str(count)+' key words fluctuation in 24 chapters')
plt.legend()
plt.show()
```

![Markdown](http://i4.bvimg.com/640680/c79d691ce875618e.png)
## 进一步分析
### 定义关键词之间的关系
之前做的是一些基本的分析，我又思考了一下，能不能怎样表示一下两个关键词之间的关系呢？因为时间仓促，我也没有查找资料，就自己定义了某种衡量方法：

想衡量两个关键词的关系，以邓小平和毛泽东为例，他们分别出现了四千多次和两千多次，分布在全书中的各个位置，我想看他们的关系，就是看他们是否会出现的比较近，或者很多时候没有什么关系。于是我考虑去计算两个关键词的“**最近邻距离**”。接下来就是如何定义这个最近邻距离。因为两个关键词的数量不一致，以个数少的作为基准，已经可以知道这个词语在我生成的词汇表的具体位置，因此我分别找到毛泽东出现的两千多个位置，然后搜索每个位置最近的邓小平这个词汇出现的位置，然后获得他们的距离。这样就可以衡量出两个关键词在每个位置的最近距离了。

虽然听起来这个过程十分的繁琐，需要大量的搜索，但是通过把循环和搜索问题变成矩阵的运算（反正位置都是数字），就可以非常快地计算出任意两个关键词的距离分布了，我给定义成了**calculate_distance**函数。

```
def calculate_distance(ind1,ind2):
    pos1 = np.where(wordlist==namelist[ind1])[0]
    pos2 = np.where(wordlist==namelist[ind2])[0]
    num1 ,num2 = pos1.shape[0],pos2.shape[0]
    if num1>num2:
        small = num2
        large = num1
        lararr = pos1
        smarr = pos2
    else:
        small = num1
        large = num2
        lararr = pos2
        smarr = pos1
    disarr = np.ndarray([small,large])  #each line calculate the small set's ith word's and large set's every words distance
    arr1= np.repeat(smarr,large).reshape(-1,large)
    arr2= np.repeat(lararr,small).reshape(-1,small).T
    mindis = np.min(np.abs(arr2-arr1),axis=1)
    return mindis

def draw_dist_count(ind1,ind2):
    fig,ax=plt.subplots(1,figsize=(20,10))
    ax.bar(range(calculate_distance(0,1).shape[0]),calculate_distance(0,1),color='g')
    ax.set_title('Minimum Distance of '+namelist[ind1]+" and "+namelist[ind2])
draw_dist_count(0,1)
```
#### 绘图
这里就拿邓小平和毛泽东两个关键词举例，我按照顺序画了出来，毛泽东出现的两千多次里，每个毛泽东与最近的一个邓小平的位置距离。值越小说明这两个关键词越靠近，要是值为1的话就说明他们挨着（不过对于名词来说一般中间至少隔着一个介词）。**这样就可以看到任意两个关键词的关系随书的文字的紧张的变化情况。**

可以看到700到1400左右，两个词的距离明显较近，说明在这部分文字中，两人发生了更为密切的联系，而500左右的距离有的非常远，说明这部分是各讲各的故事，两个人还没有交集。

![Markdown](http://i4.bvimg.com/640680/a421383dc7da2619.png)

同样的调用计算距离和绘图的函数，可以查看任意两个关键词的距离并按顺序绘制其值。

下面一次性展示了同一个关键词和其他好几个关键词的距离图。


```
fig,ax=plt.subplots(4,2,figsize=(20,20))
for i in range(4):
    for j in range(2):
        ax[i,j].bar(range(calculate_distance(0,1+2*i+j).shape[0]),calculate_distance(0,1+2*i+j))
        ax[i,j].set_title('Minimum Distance of '+namelist[0]+" and "+namelist[1+2*i+j])
```
![Markdown](http://i4.bvimg.com/640680/4414d03a5b228e77.png)

#### hist plot
接下来还画了一下距离的**分布图**，就是把上面的图中的距离统计一下他们的分布。

```
fig,ax=plt.subplots(4,2,figsize=(20,20))
for i in range(4):
    for j in range(2):
        ax[i,j].hist(calculate_distance(0,1+2*i+j),bins =50,color='b',alpha=0.4)
        ax[i,j].set_title('Minimum Distance of '+namelist[0]+" and "+namelist[1+2*i+j])
```
![Markdown](http://i4.bvimg.com/640680/3c6048942654bc2c.png)

这种图感觉就丢失很多信息了，看不出来随着书籍的发展，两个名词的关系的变化。当然如果做得更细致，可以用某些指标刻画一下这种距离图，更好地衡量两个指标的关系，用可视化的方法当然是更直观的。


#### boxplot
最后是**Boxplot**，这是另一种直观显示距离分布的图。
```
dist_data = {}
for i in np.arange(1,20):
    dist_data[i] = calculate_distance(0,i)
dataframe_dxp = pd.concat((pd.DataFrame({namelist[i]:dist_data[i]}) for i in np.arange(1,20)),axis=1)
import seaborn as sns
fig, ax = plt.subplots(figsize=(100,20))
sns.boxplot(data =dataframe_dxp,ax=ax,boxprops=dict(alpha=.5),color='g')
ax.set_title(u'Dengxiaoping and others',fontsize=80)
ax.set_xticks(range(19))
ax.set_xticklabels(namelist[1:20],fontsize=80)
fig.savefig('boxplot.png')
```
![Markdown](http://i4.bvimg.com/640680/5ef0af735a848eca.png)

可以看到邓小平和好几个关键词的距离的分布，每一个box就是一个分布的统计，当然也和分布图一样，这样一画就**拉平了**不同关键词之间的差异了。

其实对文本挖掘还有**词性标注、情感分析**等更多方法，包括归纳段落或篇章的主题等等，目前都有很多统计模型和机器学习方法可以做。不过在尝试的过程中，我还是感觉到这只是很基本的辅助方法，更重要的还在于人文历史政治学科的专家们对书籍做仔细的解读，**挖掘历史细节中的关键信息是人最擅长的**，比机器强大的多的地方，不过在卷帙浩繁的历史典籍中，面对成千上万的书籍时，快速挖掘书籍的要点，分析出来一些有趣的东西，也许机器能够帮上大忙。



# 附：读后感
这部分明显能感觉到自己笔触的迟滞和笨重，真的是很久不写这种风格的论文，表达得略显凌乱，也有可能是最近总是熬夜，写的时间也正是脑子乱乱的时候，又没有一个规范约束自己，因此写的相当不守规矩。

这次读后感分成两个部分，一个是常规的读后感想，另一部分是出于兴趣所做的一些对邓小平时代一书的基本的文本挖掘。

第一部分是我在读书时的一些感悟，尤其是比较了一些香港版和大陆版的不同之后，也产生了一些感悟。

经过查询，我发现大陆版正文较港版删节约5.3万字，其中包括“邓小平时代的关键人物”一文约2.6万字。这部分其实还相当有趣，我还专门仔细找了其中提到的几个人物的一些更多的史料，发现能读到很多令人震惊的，被有意掩盖的历史。大陆版的几个大篇幅忽略的内容包括天安门事件、邓小平南巡前后对改革停滞的不满、邓小平子女的腐败传闻等敏感话题并未避而不谈。其实这几部分也不是秘密了，我想有一些想法的民众也都能从各种地方搜集来蛛丝马迹，但是从删节中还是能感觉到一条清晰的审查红线。如果删除的内容更真实、接近历史的真相的话，确实相当让人开阔眼界，比如叶剑英病重，邓小平并未去看望，毛泽东对周恩来的打压讨厌却又离不开，在其死后冷漠的态度。这就很颠覆大家的“被培养起来的观念”，还有比如印象中对陈云和邓小平在经济问题上很好地合作的观念，也被作者纠正：陈云比起邓小平的大刀阔斧，要保守稳健很多，因此产生很多分歧，比如不去广东视察。另一个肯定被删（没有核实）的就是南巡时候的珠海会议，不留情面地批评了有点向往毛泽东的观念的江泽民，甚至威胁要取代他。这种分歧的事情大陆版应该也不会保留的。至于89年的风波，就没有什么可讨论的啦。

我总感觉，一字之差可能一篇文章意义就全变了，中国人最懂笔义春秋之法，也就格外注意这点。邓小平时代确实披露了很多史料，让中国人有机会了解自己的事情（听起来有点滑稽），但是在搜索的过程中，还是难免注意到，香港版的邓小平时代在好几个网站被列为“大陆禁书”。可想而之其中的一些关键问题，可能被隐去了。为什么会被隐去呢，我想我自己有些体会，当我初中读到上上任领导人的传记中的部分内容，感到一阵阵的震惊、震撼、深思、迷茫和一种成长成熟的不愉悦感的时候，以及日后有意无意读到搜到的各种真假故事的时候，都会隐隐体会到为什么有些历史和真实被隐去了。

读历史和任务传记，尤其是夹杂在历史洪流中的人物群像时，最让人不适的阅读体验之一就是作者对任务的评价不得不因为篇幅所限而显得相当“专断”，比如“雖然趙紫陽做人和藹可親，但一些同事認為他有點兒不合群，喜歡為自己著想。文革開始時趙紫陽讓他的部下抵抗紅衛兵，可是令部下氣憤的是，趙本人很快就把自己辦公室的鑰匙交給了紅衛兵。”这样的话语，让人很难真正对赵紫阳这样一个略显神秘的人物有更多的了解和判断，读起来还是一团历史的迷雾，当然对历史做价值判断本来就是危险且未必能断得清楚的，而如果能尽可能多得了解到公正的史料，也许就能想的更清楚一点，更全面一点，更多地祛除个人的感情，比如作者就并没有站在自由派的立场上一边倒地吹捧赵紫阳，而是指出了他自私自我的一面，这样的词语可能还是可以甚至有点受到大陆的欢迎的，而相反的，喜欢报复、邪恶狡猾之类很难见到的形容毛泽东的词语当然被删除干净。我相信人是无比复杂的，也绝不是多么高尚的，无私的，一个忘我的奉献一切的无私的人就真的是像神一样的所有人应该努力追逐的样子吗，我觉得不是，要说刨去了物质欲望，无私的奉献、为祖国的奉献、也无非是精神上的享受而已，不同类型的人难以体会另外的人的享受和愉悦的点，把事情渲染地极端又美好真的是件好事吗，需要的时候就圣洁如神明，一旦出事了一个个都不干净，这样的过程一再出现的话，恐怕就得不断填补一个巨大的漏洞，或者封住所有的缺口，封住所有信息的渠道，这样好吗。

从这里想到的东西绝不是在批评当权者和领导层，而是感到作为自私的利己的人类而言也许会永远存在的现象，我们当然不会承认自己的自私，也都指责别人的自私，在这个过程中有心照不宣也有大声互斥，我想共产党有的时候让世界上的很多人抵触和担忧就在于，一定要极端地宣称某些事情，有的时候大家都心照不宣的事情，可以揭露的事情，一定要坚持极端地说出来，这样未必是好的。然而反过来想，就算允许人们议论，揭短，又有何意义呢，对国家发展、经济增长、人民福利有什么意义呢？一下子也许还有很多负面的例子，也许很多时候我们就是这样安慰自己来进行善意的谎言的过程吧。有的时候一次性揭开历史的各种真相也未必是好事，对大多数人来说会是一个失去信仰的，难以接受的过程，就看美国也不过是借着民主自由人权的外衣干了很多坏事，包括这两天的叙利亚的空袭，为了自己的自私和利益，一套冠冕堂皇的理由好像自己也愿意相信，当然中国不信，中国说自己和平崛起不愿战争的说法，外国也不信，所以我们是否真的要探讨一下，话题开放的尺度究竟在哪里，“见过世面”、书读的多一点的人总是嚷嚷着开放，讲出真相，但是真的好吗，我记得也许是毛泽东曾经说过，知识分子什么都懂，就是不懂两点，吃不饱会饿，打仗会死人，我想书读得多了，很多道理也未必想的清楚，也只有毛泽东邓小平这样的人物，也许才能做一个比较好的决断？也许一个自诩读书万卷的知识分子人类良心真正有机会去管理人民的时候，也会发现这套管理是尽量好的办法？虽然也会有很多问题：我们避免不了伟人的错误，比如邓小平的全面物价改革可能造成的问题，我们避免不了无外部监督的权力带来的严重腐败，但这都源于每个人本身的缺点，虽然中国历史数千年来都因为通行一套文字而拥有巨量的经验，但是看起来还是很难解决好这样的问题。

本书让我感触颇深的另一点在于英雄人物的复杂性，这两段话让我感触极深：
~“基辛格11月訪華後，毛澤東為了與美國打交道，轉而依靠鄧小平這個在對抗蘇聯時十分堅定的人。1973年12月，鄧小平遵照指示參加了政治局批周的會議。無論在法國、在上海做地下工作期間還是1950年代初在北京一起工作時，周恩來就像鄧小平的兄長。但是毛澤東有理由希望鄧小平會和自己而不是周恩來站在一起。鄧小平在1940年代的整風運動中就站在毛澤東的一邊，周恩來卻沒有。自從1931年鄧小平被批為「毛派頭子」後，他就一直緊跟毛澤東，並在1950年代得到了毛的重用。1956年以後鄧小平成了黨的總書記，他和周恩來的關係在黨內事務上有時變得很尷尬：周恩來在黨內排名上高於鄧小平，可是他要向負責黨內日常事務的鄧小平彙報工作和接受指示。[2-82]周恩來在文革期間也沒有保護鄧小平。[2-83]~
~鄧小平心裏很清楚，「兩位小姐」會把他在批周會議上的發言彙報給毛主席。會議臨近結束時，鄧小平對周恩來說：「你現在的位置離主席只有一步之遙，別人都是可望而不可即，而你卻是『可望而可即』，希望你自己能夠十分警惕這一點。」[2-84]這些話表面上並不惡毒，卻暗藏殺機。鄧小平實際上是在暗示，周恩來想架空毛澤東，篡奪毛的地位。「兩位小姐」把鄧小平的發言和態度彙報給毛澤東後，毛非常興奮，立刻把鄧小平叫去談話。”~

我印象很深，小时候喜欢问父亲，这个人是好人还是坏人，父亲总会告诫我，不要用好坏去区分一个人。但是不用好坏去区分总会很头疼，很费脑子，让人很难受。毛泽东这个人的复杂性，很多历史事实和资料都能让人们略知一二，邓小平时代一书让我体会的更加深切，尤其是邓小平文革后期找到机会重新拾起权力阶段的故事，顺带让我了解到周恩来与毛泽东的分歧，而按顺序读起来，总是让人感慨颇深，一会儿对某人充满同情，一会儿又更加深切地感受到人物关系的复杂，让人对这些历史巨人产生了掺杂的混合的情绪，不知道同情谁好，不知道支持谁好，更别说谁是好人谁是坏人。可见评价历史真是件十分难的事情，如果客观地叙述事实，就只是记录罢了，而历史终归要掺入主观的观念，从这点上看，我反而觉得不要删节，让有心的读者多读读更好，为什么要把人维护得如此正面而甚至虚假呢，让人更加深切地体会到人性、人生和历史的复杂，更加谨慎地获取和得到自己的观点，岂不是更好？
	
大陆版删除的关键人物一章，读起来相当有意思，虽然在我看来这一章里篇幅所限也并没有很细致的描述，可能是不希望人们太多的关注人物背后的交情以及过多思考人性的复杂，故而这章也被删除了，我觉得其中一些写的不错，包括读到了赵紫阳的个人性格和可能带来的局限性，比如过分自爱，性格较为保守，对于管理经济倒是非常擅长。从很多人物和邓小平的关系看来，邓小平确实在管理人，尤其是管理领袖级人物上非常有一套，虽然不及毛泽东的本事，把开国功臣们掌握得牢牢的，但是也能不需要居于最前方就可以实际掌控大局，我觉得邓小平在很多地方上是借鉴了毛泽东的，而且去除了一些掌控欲，也有可能是由于一代英雄们都已经谢幕，相对掌控起来也更加容易，让邓小平有机会居于稍微靠后的位置，就牢牢掌控着一切局势，而且我感觉他和毛泽东后期一样，在挑人上都有意选择一些并未在权力中心浸染很久的人，比如王洪文、华国锋、赵紫阳和江泽民等等，赵紫阳是他出访尼泊尔路过四川一番交谈最终调到北京，而江泽民，我记得在各处看到，大概是邓小平喜欢春节在上海度过，加上八九年的风波，江泽民处理得很好，就空降到政治局成了领导者。这种选人的风格给邓小平带来了很多主动性，也许也有未来避免在历史上少背点锅的可能也说不好。做实验的过程中，都特别强调试探，邓小平可以多一些试探的余地，更多地观察再做决定，加上自己实际的军队和政治上的掌控权和表面领导人的生疏，可以让自己牢牢地掌管真正的核心决策。我记得江泽民曾经对毛泽东的一套更加喜欢，对市场化的进程感到过快，邓小平于是南下巡防，声称反对改革开放就是反对社会主义，由此压制了另一套想法的滋生。

但是这种掌管权力的方法也未必是完全好的，想想邓小平其实在一个相当“美好”的历史时期，活到最后的就是胜利者，邓小平73年说自己还能干二十年，结果真的干到十四大退到幕后，这种后发优势熬倒了很多元老，导致在大清洗之后的权力真空期获得了很多权力，恐怕接下来的两任都在某些时间段并不舒服，有很多桎梏，到这一任才有了更加集中的权力。这里面有的时候也可以看出人性的有趣之处。干部在反对个人集权的时候又希望自己能够有更多的权力，当年反对个人集权，是反对毛泽东权力过大，随意分配权力，两位小姐、造反派头头竟然能高过周恩来，反对的是权力危害到了我，而当自己可以有权力的时候，有人会愿意不要吗，恐怕也是很少见的。所以我觉得人们在意的可能更多的是权力能不能更多地为自己所用，以及其他的权力不要危害到自己，最好在一种比较好的平衡中，但是平衡也未必会永远存在，也未必是好事。看起来邓小平用权力的集中做了很多伟大的改革，毛泽东也做出了一系列彪炳千秋的伟业和载入史册的糊涂事，如今权力也更加集中起来，如果思路明晰的话，我觉得权力集中是能够做大事的，否则光是吵吵嚷嚷争论不休就什么都做不了了。只是权力集中虽然有可能带来很多的回报，也有更大的风险，争吵不休而止步不前面临的是缓慢的毁灭，一意孤行地推进则有走向荣光和加速衰败两种更加极端的选择。

这本书讲了很多改革和开放的故事，看起来是讲述了一个政党的领导人如何自我改革，把自己变得更加兼容并包，在术的层面不断学习进步的过程，但是大胆地一说，总是能感觉到在政治和管理问题上，管理者有极其强烈的控制欲，这当然很重要，必须承认，如果不是这样，也许我们已经几次陷入严重到可以亡国的风险，这种经验教训可能也让共产党更加谨慎小心，让改革开放这个词语显得更加多面性：从西方的角度，我们的所谓改革开放，也就是做到人家常规的程度，甚至都远远不够，而共产党在这个过程中要不断地审视各种局势，更加小心翼翼。我的感觉是，共产党相比于其他政党，更喜欢和强调管教，但是又不是喜欢按照法律管理，而是希望有更大的自由裁定权，记得一位高层曾经说过，法律不要立的太细致，这样才有解释的空间。管制与压抑的原理是什么？这恐怕是很多人心中的无法言说的疑问，不管会乱，有很多例子，所以我们就愿意并满足于上交更多的权利？比如自由获取信息、翻阅墙壁、表达言论的权利。某些做法使人很容易往不好的一面联想，虽然这也许并非管理者与人民的本意，但是实在是与神圣纯洁的宣传所矛盾。
	
最近对快手、抖音、内涵段子等的查封，以及其“段友”、“抖友”发展出来的有一定结构的组织，让管理者更加紧绷，很多人说这些地方都是垃圾，价值观歪曲，封的好，但是也有人心中充满了疑问，以及经典的“他们向…发难，而我没有发声，最终他们向我而来，没有人替我说话了”的担忧。管理者的这种一贯的，一刀切的、有点精神洁癖和完美主义者喜欢的掌控感与清净感让人赞许又害怕，让包括我在内的很多人处于矛盾：激动并自豪于中国人的成就、又觉得这来源于中国人传统的吃苦与奋进，又觉得这是对自己大大压抑和压榨的结果，觉得我们并没有跳出某种循环，让人迷茫于很多事情和自己的意义，感慨于人类历史和经验的复杂和不足够：看不清楚究竟什么是正确的路，估计不出来自己和国人和世界所处的情况、条件等等。

以上已经做了很多带有敏感词的评论，在我看来邓小平无疑是个奇人，尤其是作为开国领袖级人物，在分割明显的76年之后，又在领袖群体凋落的时代支撑并扭转了中国的大势，这是千秋功业，邓小平在73年的时候说出自己还能再干二十年，实在是几代人的幸运。


