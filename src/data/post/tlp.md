---
title: "On Wittgenstein's Picture Theory of Meaning"
publishDate: 2018-05-03T23:19:27Z
description: ""
author: "James Chen"
tags: ["philosophy", "Wittgenstein", "thoughts"]
category: "thoughts"
draft: false
---


#### My first naive understanding of picture theory:
I feel that I am always very slow at catching up with ideas in philosophers’ minds. In my child I am slow at understanding, now I always feel that I have more doubt about the ideas to restrict me to understand the ideas. So I try to understand the picture theory by reading some Chinese’s  thoughts. Then read more thoughts in English.

**PDF version**:
<iframe src="https://drive.google.com/file/d/1TIhBW_rq754AtFNdfnS0HfbjznXGSdvY/preview" width="100%" height="600px"></iframe>


In Wittgenstein’s early thinking — from the TLP era — he conceived of the world in terms of facts rather than objects (from Russel’s Logical Atomism). Objects are simple: a ‘red ball’ is an object that has the property of ‘redness’ and an assortment of properties that are associated with ‘ball-ness’. Facts, by contrast, are states of affairs that involve relationships between objects. e.g.: A ‘red ball’ is an object,
‘John sees a red ball’ is a fact, drawing a relationship between an object ‘John’ and an object ‘red ball.’
Facts are carried by propositions in language — the proposition “John sees a red ball” conveys the fact that John sees a red ball — and propositions are pictures of the state of affairs that is being conveyed. Like visual pictures, propositions can be created that are more detailed or less detailed, that highlight some relationships and downplay or omit others, that present idealized forms of facts or realistic forms. Thus, I can say:

- “James sees a red ball”
- “The boy James sees a bouncing red playground ball”
- “James sees the redness of the ball in the setting sun”

These are three ways of drawing slightly different logical pictures of the basic fact in point 1. Wittgenstein uses this as a way of taking about the relationship of language to the real world; it is a correspondence theory of language.

On Wittgenstein's view, the world consists entirely of facts. (Tractatus 1.1), *The world is the totality of facts, not of things.* Human beings are aware of the facts by virtue of our mental representations or thoughts, which are most fruitfully understood as picturing the way things are. (Tractatus 2.1),*We make to ourselves pictures of facts.* These thoughts are, in turn, expressed in propositions, whose form indicates the position of these facts within the nature of reality as a whole and whose content presents the truth-conditions under which they correspond to that reality. (Tractatus 4),*The thought is the significant proposition.* Everything that is true—that is, all the facts that constitute the world—can in principle be expressed by atomic sentences. Imagine a comprehensive list of all the true sentences. They would picture all of the facts there are, and this would be an adequate representation of the world as a whole.

#### Is it a trick to use picture?
Wittgenstein argues that propositions, as facts are images, just like musical scores are images of music or letters are images of spoken language. In other words, from an image point of view, the propositional relationship to reality, the relationship between music scores and music, and the relationship between pronunciation symbols and spoken language are exactly the same.   They are all images. And, for this kind of image relationship, it is understandable by recalling hieroglyphs. Because the alphabetic characters evolved from hieroglyphics, as an image, its essence is still hieroglyphs. This view is equally suitable for music score as an image and proposition as an image. Thus, Wittgenstein’s logical image theory is essentially the same as the pictographic character. What is different is that Wittgenstein uses the word “hieroglyphs” in the original sense, not metaphorically.

To Wittgenstein, the proposition only shows the fact that the mark is just a mark object, just like a pictogram representing the object that symbolizes it. In simple terms, the image is only a symbolic relationship, not a reflection relationship. On the other hand, images depict their own objects from outside as if the hieroglyphs describe the shape of their own objects. So between the proposition and the fact, the sign and the fact are only similar in appearance. Image-based theory is a kind of pictographic theory. Therefore, Wittgenstein's logic image theory has the essential characteristics of formalism and agnosticism. 

It suddenly occurs to me that Wittgenstein is really a genius by defining the relationship of proposition and fact by pictures. It seems that he always has the most genius institution to simplify complex and convoluted mysterious. He tries to solve a very very hard problem involving language (and mind) by turning into a picture/plot/image problem. Let’s just have a look about their differences using the view nearly 100 years ago:

##### AI’s view:

----
----
We should talk about some details of AI’s achievement and problems to understand Wittgenstein’s insight.

AI has a long history, but the event made AI back to public’s eyes is AlphaGO beats the one of the best Go players in spring in 2016. But the AI revives happened a little earlier in the academic area. At around 2012 a brand new method called deep learning finally became mature and beats all the other models in computer vision competition. The competition aims at recognize and classify the picture. For example: a child at 3 years old can recognize a painting and point out that there is a cat on it. But it is
impossible for computers to do so. But with the deep learning model, the computer can do the amazing work just like human being. Later more and more work and achievements got into public’s view. We can see AI everywhere: automatic driving, face recognition and machine translation. And the concerning about AI will do some harmful things like replace human and kill human when they obtain the intelligence.

So will they? I’d like to talk about another famous example called Turing test. It is proposed by Allen Turing to accurately definite the actual artificial intelligence. It can be simply said like: a man is outside of a room, the room may contain either a human or a computer, but the man outside of the room doesn’t know. Then they will communicate through a screen. And after some kind of time(maybe 25 mins), the man is asked to tell if the man in the room is a actual man or a computer. If the computer cheated the man and made him think that it is ‘a man or woman’, then we will say the computer passed the Turing test. Unfortunately there is no computer passed the Turing test yet(though there are plenty of them claims they had, but they use some tricks which is no admitted by the other scientists.)

So what can we learn from it? We can know that Turing believes that language is the ultimate way to examine the actual intelligence of the a so called AI. Actually today’s deep learning model has gained some name they don’t deserve. The public have many wrong ideas about it, it is actually more like the artificial idiot. It can solve so little problems. One of the hardest it still did very bad is called natural language processing(NLP). It is another very hard yet interesting area. Many scientists aims to develop good models to recognize the meaning of sentence and passage, but it is very hard. Many big companies like Google and Microsoft also hire many experts aims to solve the hardest question in AI area.

We can use some data to see it more clearly: many deep learning models can solve the image recognition problem pretty good, achieving the accuracy at around 99% which is better than human. But the NLP problem is much harder. A dataset  provided by Stanford University containing many sentences for sentiment analysis(classify the sentences into several kinds of categories according to their sentimental meanings). The state of art model on these dataset can only achieve ~50% accuracy. Which is not satisfying. Actually even human can’t do it very well comparing to image recognition. Part of the reason maybe the image contains much more information than the single sentence. And a sentence contains so many levels of meanings which is even very hard for human to recognize and understand.

So in order to achieve actual intelligence. To make AI more like human intelligence. The best practice and test is natural language problem. The language is the representation of intelligence. If we want to know about intelligence, we should know about language more. Solving language problem is surely not the language itself, the experience, custom, belief, philosophy behind it makes it really complex.

So language is really amazing and interesting and hard to understand and use. We can hope that one day the ‘AI’ gains the philosophy behind it. But I think it will happen after we gain more knowledge of language and our brain.

----
----

That is a little long thoughts about the AI’s limits. Which is really inspiring in our situation: Amazingly, machine are really good at image recognition and classification, they even get better performance than human these days. Lecun, one of the great experts in deep learning, mimic cat’s visual system in machine and make the AI as good as man in image tasks. But machines are really, really bad at natural language processing(**NLP**) tasks. We all admit that NLP is much harder than computer vision problems. Recently many researchers do some interesting application using computer vision related model(**CNN**) to solve many non-visual tasks, including **NLP**! It is quite inspiring. Since we have lots of experience and power in solving computer vision problem, the CNN model is powerful especially at extracting features hidden under everything(including picture and languages), it is natural to use image-related methods to solve language problems. After thinking about that, I am thrilled that Wittgenstein also use the same **trick**. I think he must understand how weak we are facing the language problems, but we are really good solving problems related to images. They have more features, they can extend our understanding in a more comfortable way. Especially for us ordinary people with much less intuition than him, picture the proposition as image is really helpful. It seems that Wittgenstein finds a powerful, logical tools to answer how people’s understanding can be expressed and the inherent relationship between the proposition and reality



### References
1 [Ludwig Wittgenstein: Analysis of Language](http://www.philosophypages.com/hy/6s.htm#trac)
2 Terrone, E. (2013). Wittgenstein’s Picture Theory of Pictures. Aisthesis. Pratiche, Linguaggi E Saperi Dell’Estetico, 6(1), 275-290. doi:10.13128/Aisthesis-12852
3 [Picture theory of language - Wikipedia](https://en.wikipedia.org/wiki/Picture_theory_of_language)

## Additional: Basic data mining of TLP
I also spent some time analyzing the context of TLP using python:

### prepare data
```
import numpy as np
import jieba
import os, codecs  
from collections import Counter  
import matplotlib.pylab as plt
plt.style.use('ggplot')
import pandas as pd
import matplotlib
import re
with codecs.open('text.txt', 'r', 'utf8') as f:  
    txt = f.read()
def get_words(txt):  
    seg_list = jieba.cut(txt)  
    c = Counter()  
    for x in seg_list:  
        if len(x)>1 and x != '\r\n':  
            c[x] += 1  
    return c  # c.most_common(100)
if __name__ == '__main__':  
    with codecs.open('text.txt', 'r', 'utf8') as f:  
        txt = f.read()  
    a = get_words(txt)
dat = a.most_common(1000)
countlist = np.ndarray([1000,2]).astype('str')
for i in range(1000):
    countlist[i][0] = dat[i][0]
    countlist[i][1] = dat[i][1]
wordlist = np.array(re.split(' |; |, |\*|\n',txt))
index_54 = [9,11,13,14,16,17,21,23,25,26,28,29,32,33,35,37,39,40,43,44,45,51,53,61,63,64,66,67,69,70,72,76,77,80,81,82,84,85,86,88,89,90,91,95,96,98,99,101,103,105,106,107,109,110]
countlist_ = []
for i in range(1000):
    countlist_.append(countlist[i,0]+': '+countlist[i,1])
pd.DataFrame(np.array(countlist_)[:200].reshape(20,10))
```
### words counts, most frequent

![Markdown](http://i4.fuimg.com/640680/cf8ade4c4e8d7656.png)

```
count = 27
fig,ax=plt.subplots(1,figsize=(20,10))
ax.bar(range(count),countlist[:count,1].astype('int'))
ax.set_xticks(range(count))
ax.set_xticklabels(countlist[:count,0])
ax.set_title('most frequent words')
```

![Markdown](http://i4.fuimg.com/640680/403967dbf8791a89.png)


### most frequent words with specific meaning
```
count = 30
index = index_54[:count]
fig,ax=plt.subplots(1,figsize=(20,10))
barlist = plt.bar(range(count),countlist[index,1].astype('int'))
for i in np.arange(0,2):
    barlist[i].set_color('r')
for i in np.arange(2,5):
    barlist[i].set_color('b')
for i in np.arange(5,12):
    barlist[i].set_color('g')
for i in np.arange(12,26):
    barlist[i].set_color('m')
for i in np.arange(26,count):
    barlist[i].set_color('y')
ax.set_xticks(range(count))
ax.set_xticklabels(countlist[index,0],fontsize=15)
ax.set_title('Most Frequent Words in TLP',fontsize=30)
plt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')
fig.tight_layout()
```

![Markdown](http://i4.fuimg.com/640680/1488a2a32bc0e559.png)

### words frequency by chapter
```
chapterind = [460,559,2285,4855,10438,17882,22970]
namelist = countlist[index,0]
def count_frequent(chap,count):
    freqlist =[]
    if chap <6:
        for i in range(count):
            freqlist.append(np.where(wordlist[chapterind[chap]:chapterind[chap+1]] ==namelist[i])[0].shape[0])
    else:
        for i in range(count):
            freqlist.append(np.where(wordlist[chapterind[chap]:] ==namelist[i])[0].shape[0])
    return np.array(freqlist)
freq_var = np.ndarray([7,30])
for i in range(7):
    freq_var[i] = count_frequent(i,30)
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
transformed = scaler.fit_transform(freq_var)
fig,ax=plt.subplots(1,figsize =(10,10))
ax.matshow(transformed.T ,cmap ='jet')
ax.set_title('30 key words fluctuation in 7 chapters')
ax.set_xticks(range(7))
ax.set_yticks(range(30))
ax.set_yticklabels(namelist)
```

**You can see the words variation in different chapters**. For example, sign appears most in chapter 3, the chapter talking about the logical picture of facts.  In another example you can see picture appears most in chapter 2. Since chapter four is the longest chapter, most words appears more times in it.

![Markdown](http://i4.fuimg.com/640680/0caf8e3a241751bd.png)


```
fig,ax=plt.subplots(1,figsize =(20,10))
#ax.plot(freq_var[:,:10])
count =10
for x,y in zip(freq_var[:,:count].T,namelist[:count]):
    plt.plot(x,label =y)
plt.title(str(count)+' key words fluctuation in 24 chapters')
plt.legend()
plt.show()
```
It is another plot stating the same fact as the previous heatmap.

![Markdown](http://i4.fuimg.com/640680/dc2b19befbe552ac.png)

### Calculate different words’ distances
I also think about how to depict different words’ relationship. One easiest way I think of is calculating two words minimum distance in the book whenever they appears. **For example:**
```
def calculate_distance(ind1,ind2):
    pos1 = np.where(wordlist==namelist[ind1])[0]
    pos2 = np.where(wordlist==namelist[ind2])[0]
    num1 ,num2 = pos1.shape[0],pos2.shape[0]
    if num1>num2:
        small = num2
        large = num1
        lararr = pos1
        smarr = pos2
    else:
        small = num1
        large = num2
        lararr = pos2
        smarr = pos1
    disarr = np.ndarray([small,large])  #each line calculate the small set's ith word's and large set's every words distance
    arr1= np.repeat(smarr,large).reshape(-1,large)
    arr2= np.repeat(lararr,small).reshape(-1,small).T
    mindis = np.min(np.abs(arr2-arr1),axis=1)
    return mindis
def draw_dist_count(ind1,ind2):
    fig,ax=plt.subplots(1,figsize=(20,10))
    ax.bar(range(calculate_distance(0,1).shape[0]),calculate_distance(0,1),color='g')
    ax.set_title('Minimum Distance of '+namelist[ind1]+" and "+namelist[ind2])
draw_dist_count(0,1)
```

It is the minimum distance of two words: **picture and objects**, we can see at the first half of the book, the two words have closer distance, which means they have higher chance appearing together or nearby. But in the last half, apparently the author didn’t arrange them  appearing together.

![Markdown](http://i4.fuimg.com/640680/dd3d1fca192d56eb.png)

We can also see one word’s distance with many other words: **logical and other words**
```
fig,ax=plt.subplots(4,2,figsize=(20,20))
for i in range(4):
    for j in range(2):
        ax[i,j].bar(range(calculate_distance(2*i+j,8).shape[0]),calculate_distance(2*i+j,8))
        ax[i,j].set_title('Minimum Distance of '+namelist[9]+" and "+namelist[2*i+j])
```
![Markdown](http://i4.fuimg.com/640680/88bdbaae024ff88c.png)


We can also overview one word’s distance(relationship) with others using hist or boxplot
```
fig,ax=plt.subplots(4,2,figsize=(20,20))
for i in range(4):
    for j in range(2):
        ax[i,j].hist(calculate_distance(0,1+2*i+j),bins =50,color='b',alpha=0.4)
        ax[i,j].set_title('Minimum Distance of '+namelist[0]+" and "+namelist[1+2*i+j])
```

![Markdown](http://i4.fuimg.com/640680/0ff4b00256b64e7a.png)

```
dist_data = {}
for i in np.arange(1,20):
    dist_data[i] = calculate_distance(0,i)
dataframe_dxp = pd.concat((pd.DataFrame({namelist[i]:dist_data[i]}) for i in np.arange(1,20)),axis=1)
import seaborn as sns
fig, ax = plt.subplots(figsize=(100,20))
sns.boxplot(data =dataframe_dxp,ax=ax,boxprops=dict(alpha=.5),color='g')
ax.set_title(u'Proposition and others',fontsize=120)
ax.set_xticks(range(19))
ax.set_xticklabels(namelist[1:20],fontsize=80)
plt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')
fig.tight_layout()
```

![Markdown](http://i4.fuimg.com/640680/d2e029996f53858b.png)

That’s all about my basic data mining and analysis of TLP. It took me a lot of time writing codes to count and plot. I believe in the future we can use data mining, natural language processing to do analysis more automatically. What’s more, it may do some really exciting and serious study instead of my basic plotting. We may use the powerful so called “artificial intelligence” tool to mine thousands of books and materials to analyze the hidden ideas which is omitted by human due to our limitation of memorizing. Thus we can understand more about the author and the ideas behind the book.





